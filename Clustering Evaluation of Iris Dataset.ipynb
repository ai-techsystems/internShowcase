{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"```{r}\nlibrary(ggplot2)\nlibrary(readr)\n```\n\n```{r}\nrm(list = ls()) ; gc()\nhead(iris)\ntail(iris)\nstr(iris)\nsummary(iris)\nstr(iris$Species)\nsummary(iris$Species)\ndim(iris)\n```\n\n## Method 1 (Trial and error using preferred K-Means or number of Clusters)\n```{r}\nlibrary(ggplot2)\ndata <- iris[,1:4]\nstr(data)\nnames(data)\nsummary(data)\ndim(data)\nplot(data, main = \"The legth and width of Sepal and Petal\", pch =20, cex =2)\n```\n\n### Perform K-Means with 2 clusters\n```{r}\nkm1 = kmeans(data, 2, nstart=100)\nplot(data, col =(km1$cluster +1) , main=\"K-Means result with 2 clusters\", pch=20, cex=2)\n```\n\n### Perform K-Means with 3 clusters\n```{r}\nkm2 = kmeans(data, 3, nstart=100)\nplot(data, col =(km2$cluster +1) , main=\"K-Means result with 3 clusters\", pch=20, cex=2)\n```\n\nOne solution often used to identifiy the optimal number of clusters is called the Elbow method and it involves observing a set of possible numbers of clusters relative to how they minimise the within-cluster sum of square.\n\n```{r}\nrm(list = ls()) ; gc()\ndata <- iris[,1:4]\ntot_wss<-c()\nfor (i in 1:15)\n{\n  cl <- kmeans(data, centers=i)     \n  tot_wss[i]<-cl$tot.withinss     \n}\n```\n\n```{r}\nplot(x=1:15,                         \n     y=tot_wss,                      \n     type=\"b\",                      \n     xlab=\"Number of Clusters\",\n     ylab=\"Within groups sum of squares\")\n```\n\nFrom the above scree plot, we can say that after 2 clusters the observed difference in the within-cluster dissimilarity is not substantial. Consequently, we can say with some reasonable confidence that the optimal number of clusters to be used is 2.\n\n## Method 2 (Clustering based on \"NbClust\" package)\nProcess to compute the “Elbow method” has been enhanced by using NbClust package. This library provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n```{r}\nrm(list=ls()) ; gc()\nlibrary(factoextra)\nlibrary(NbClust)\ndata<-iris[,-c(5)]\npar(mar = c(2,2,2,2))\nnb <- NbClust(data, method = \"kmeans\")\n```\n```{r}\nhist(nb$Best.nc[1,], breaks = 15)\n```\n```{r}\nfviz_nbclust(nb) + theme_minimal()\n```\n\nBased on the result above; 2 proposed  0 as the best number of clusters, 10 proposed  2 as the best number of clusters, 8 proposed  3 as the best number of clusters, 2 proposed  4 as the best number of clusters, 1 proposed  5 as the best number of clusters, 1 proposed  8 as the best number of clusters, 1 proposed  14 as the best number of clusters, 1 proposed  15 as the best number of clusters. Thus, according to the majority rule, the optimal number of clusters is 2.\n\n```{r}\nfviz_nbclust(data, kmeans, method = \"wss\") + geom_vline(xintercept = 2, linetype = 2)\n```\n\n## Method 3 (Clustering based on \"vegan\" package)\n\n```{r}\nrm(list = ls()) ; gc()\nlibrary(vegan)    \ndata<-iris[,-c(5)] \nmodel <- cascadeKM(data, 1, 10, iter = 100)\nplot(model, sortg = TRUE)\n```\n\nGroup membership is indicated by color. Y-axis indicates number of clusters or groups, meanwhile X-axis indicates number of objects. Based on Calinski criterion, the best number of clusters is 3 and this group membership is reperesented by 3 colors (orange, yellow, red).\n\n```{r}\nmodel$results[2,]   \nwhich.max(model$results[2,])\n```\n\n## Method 4 (Clustering based on \"cluster\" package or Silhoutte analysis)\nSilhouette refers to a method of interpretation and validation of consistency within clusters of data. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. \n\n### Comparing 2 Clusters\n```{r}\nrm(list = ls()) ; gc()\nlibrary(cluster)\nhead(iris,2)\ncl <- kmeans(iris[,-5], 2)\ndis <- dist(iris[,-5])^2\nsil = silhouette (cl$cluster, dis)\nplot(sil, main = \"Silhoutte Analysis of Iris Data Set\", col = c(\"yellow\", \"red\"), ylab=\"Number of Clusters\", xlab=\"Silhouette Range or Distance\")\n```\n\n### Comparing 3 Clusters\n```{r}\nrm(list = ls()) ; gc()\nlibrary(cluster)\nhead(iris,2)\ncl <- kmeans(iris[,-5], 3)\ndis <- dist(iris[,-5])^2\nsil = silhouette (cl$cluster, dis)\nplot(sil, main = \"Silhoutte Analysis of Iris Data Set\", col = c(\"green\", \"blue\", \"purple\"), ylab=\"Number of Clusters\", xlab=\"Silhouette Range or Distance\")\n```\n\n### Comparing 5 Clusters\n```{r}\nrm(list = ls()) ; gc()\nlibrary(cluster)\nhead(iris,2)\ncl <- kmeans(iris[,-5], 5)\ndis <- dist(iris[,-5])^2\nsil = silhouette (cl$cluster, dis)\nplot(sil, main = \"Silhoutte Analysis of Iris Data Set\", col = c(\"yellow\", \"red\", \"green\", \"blue\", \"purple\"), ylab=\"Number of Clusters\", xlab=\"Silhouette Range or Distance\")\n```\n\nThere is a summary measure at the bottom of the plot labeled \"Average Silhouette Width\". This indicates Silhouette Coefficient (SC) and the below table shows how to use the value. Based on the Silhoutte analysis, when the data was divided into two and three clusters, the Average Silhoutte Width was 0.85 and 0.74 respectively. However, it is more appropriate to divide tha data into two clusters because each individual clusters (Cluster 1 and Cluster 2) are having higher Silhouttte Coefecienet (> 0.80).:\n\nRange of SC | Interpretation\n------------- | -------------\n0.71-1.00 | A strong structure has been found\n0.51-0.70 | A reasonable structure has been found \n0.26-0.50 | The structure is weak and could be artificial\n< 0.25 | No substantial structure has been found \n\n\n## Assessing Clustering Tendency\nThe process of assessing clustering tendency or the feasibility of the clustering analysis is fundamental because this will decide whether any dataset needs to be clustered.\n\n```{r}\nrm(list = ls()) ; gc()\nlibrary(ggplot2)\nlibrary(factoextra)\nlibrary(clustertend)\n```\n\n```{r}\n# Iris data set\nstr(iris)\ndf <- iris[, -5]\n```\n\n```{r}\n# Random data generated from the iris data set\nrandom_df <- apply(df, 2, function(x){runif(length(x), min(x), (max(x)))}) \nrandom_df <- as.data.frame(random_df)\n```\n\n```{r}\n# Standardize the data sets\ndf <- iris.scaled <- scale(df)\nrandom_df <- scale(random_df)\n```\n\n```{r}\n# Compute Hopkins statistic for iris dataset\nres <- get_clust_tendency(df, n = nrow(df)-1, graph = FALSE)\nres$hopkins_stat\nhopkins(df, n = nrow(iris) -1)\n```\n\n```{r}\n# Compute Hopkins statistic for a random dataset\nres <- get_clust_tendency(random_df, n = nrow(random_df)-1, graph = FALSE)\nres$hopkins_stat\nhopkins(random_df, n = nrow(iris) -1) \n```\n\nIt can be seen that iris dataset (df) is highly clusterable (the H value = 0.19 which is far below the threshold 0.5). Meanwhile, the random dataset (random_df) is not clusterable (the H value = 0.50). This finding is consistent with visual assessment of clustering tendency which was computed using dissimilarity matrix, as shown below. The dissimilarity matrix image confirms that there is a cluster structure in the iris dataset but not in the random dataset.\n\n```{r}\nfviz_dist(dist(df), show_labels = FALSE) + labs(title = \"Iris data\")\nfviz_dist(dist(random_df), show_labels = FALSE) + labs(title = \"Random data\")\n```","execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}