{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# IMPORTING THE REQUIRED LIBRARIES\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\nfrom nltk import FreqDist\nimport string\nimport spacy\nnlp= spacy.load(\"en\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTING THE DATA\ndata = pd.read_csv(\"../input/fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['language'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby([\"type\"]).size().plot(kind=\"barh\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt= data[\"title\"][1009]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(txt)    \nolist = []\nfor token in doc:\n    l = [token.text,\n        token.idx,\n        token.lemma_,\n        token.is_punct,\n        token.is_space,\n        token.shape_,\n        token.pos_,\n        token.tag_]\n    olist.append(l)\n    \nodf = pd.DataFrame(olist)\nodf.columns= [\"Text\", \"StartIndex\", \"Lemma\", \"IsPunctuation\", \"IsSpace\", \"WordShape\", \"PartOfSpeech\", \"POSTag\"]\nodf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc =nlp(txt)\nolist= []\nfor ent in doc.ents:\n    olist.append([ent.text,ent.label_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"odf = pd.DataFrame(olist)\nodf.columns = [\"Text\", \"EntityType\"]\nodf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Performing noum phrase chunking using spacy\n   * Phrase chunking means noun plus the words describing the noun"},{"metadata":{"trusted":true},"cell_type":"code","source":"txt= data[\"title\"][2012]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc =nlp(txt)\nolist= []\nfor chunk in doc.noun_chunks:\n    olist.append([chunk.text, chunk.label_,chunk.root.text])\nodf =pd.DataFrame(olist)\nodf.columns=[\"NounPhrase\",\"Label\",\"RootWord\"]\nodf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dependency Parser\n\nA dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads - Stanford NLP\n\nSpacy can be used to create these dependency parsers which can be used in a variety of tasks."},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(data[\"title\"][1009])\nolist = []\nfor token in doc:\n    olist.append([token.text, token.dep_, token.head.text, token.head.pos_,\n          [child for child in token.children]])\nodf = pd.DataFrame(olist)\nodf.columns = [\"Text\", \"Dep\", \"Head text\", \"Head POS\", \"Children\"]\nodf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"from scipy import spatial\nnlp = spacy.load('en_core_web_lg')\ncosine_similarity = lambda x,y: 1-spatial.distance.cosine(x,y)\nqueen = nlp.vocab[\"Queen\"].vector\ncompound_simi = []\nfor word in nlp.vocab:\n    if not word.has_vector:\n        continue\n    simi = cosine_similarity(queen, word.vector)\n    compound_simi.append((word, simi))\n\ncompound_simi = sorted(compound_simi, key=lambda item: -item[1])\nprint([w[0].text for w in compound_simi[:10]])\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[1,[\"title\",\"text\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.title.fillna(\"\", inplace=True)\ndata.text.fillna(\"\", inplace=True)\nall_text = data.title.str.cat(data.text, sep=' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words= nltk.word_tokenize(\" \".join(all_text.tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport string\nstop_w= stopwords.words(\"english\")\n#cleanwords = [i for i in words if i not in stop_w and i.isalpha() and len(i) > 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nwordcloud2 =WordCloud(\n                    stopwords= STOPWORDS,\n                    background_color= \"white\",\n                    width= 1200,\n                    height= 1000,\n                    ).generate(\" \".join(cleanwords))\nplt.imshow(wordcloud2)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## WORKIGN WITH BIGRAMS\nbigrams = nltk.bigrams(cleanwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ncounter =Counter(bigrams)\nprint(counter.most_common(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(\"language\").size().plot(\"barh\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(data[\"language\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filtering out the most uncommon words out from the whole corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"def filter_words(word,top_k=1000,bottom_k=1200):\n    fdist= FreqDist(words)\n    top_k_words= fdist.most_common(top_k)\n    bottom_k_words= fdist.most_common(bottom_k)\n    top_f= set(zip(*top_k_words))\n    bottom_f= set(zip(*bottom_k_words))\n    return(top_f,bottom_f)\"\"\"\ntop_k=10000\nbottom_k=15000\nfdist= FreqDist(words)\ntop_k_words= fdist.most_common(top_k)\nbottom_k_words= fdist.most_common(bottom_k)\ntop_f= set(zip(*top_k_words))\nbottom_f= set(zip(*bottom_k_words))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning out the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FUNCTION FOR CLEANING OUT THE DATAFRAME'S COLUMNS WORDS\ndef dataclean(dff,words=words):\n    #top_w, bottom_w=filter_words(words)\n    table =str.maketrans(\"\",\"\",string.punctuation)\n    words= dff.split(\" \")\n    desc= [lem.lemmatize(w.lower()) for w in words if not w in stop_w]\n    #desc= [w for w in desc if w not in bottom_w]\n    #desc= [w for w in desc if w not in top_w]\n    desc= [w.translate(table) for w in desc]\n    desc= [w for w in desc if w.isalpha()]\n    desc= [w for w in desc if len(w)>2]\n    desc= \" \".join(desc)\n    return(desc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= data[data[\"language\"]==\"english\"][\"text\"].apply(dataclean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= data[data[\"language\"]==\"english\"][\"title\"].apply(dataclean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddf=x.str.cat(y,sep=\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[data[\"language\"]==\"english\"][\"type\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing the TfidfVectorizer of the simpliified dataframe\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=8000)\nX = vectorizer.fit_transform(ddf)\nfeat= vectorizer.get_feature_names()\nX= X.toarray()\nddf= pd.DataFrame(X, columns=feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FORMING THE CLUSTER USING THE GENERATED ARRAY IN TFIDF USING KMEANS ALGORITHM\nfrom sklearn.cluster import KMeans\nimport numpy as np\nkmeans = KMeans(n_clusters=2, random_state=0).fit(ddf)\nkmeans.labels_\nkmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PRDICTING THE RESULT\npred= kmeans.fit_predict(ddf.iloc[200:4000,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## REDUCING THE DIMENSIONS OF THE DATASET FOR VISUALIZATION\nimport numpy as np\nfrom sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2).fit_transform(ddf)\nX_embedded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAKING THE PLOT OF THE SCATTERED DATA\nplt.scatter(X_embedded[pred==0,0],X_embedded[pred==0,1],color=\"r\")\nplt.scatter(X_embedded[pred==1,0],X_embedded[pred==1,1],color=\"b\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}